{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "# from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_folder, transform=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self._img_paths = []\n",
    "        for path, subdirs, files in os.walk(data_folder):\n",
    "            for name in files:\n",
    "                self._img_paths.append(os.path.join(path, name))\n",
    "\n",
    "\n",
    "        # self._img_paths = [os.path.join(data_folder, f)\n",
    "        #                    for f in os.listdir(data_folder)]\n",
    "        self._target_img_size = 256\n",
    "        self._transform = transform\n",
    "\n",
    "        # self._to_tensor = T.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        selected_img_path = self._img_paths[index]\n",
    "\n",
    "        img = cv2.imread(selected_img_path)\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "\n",
    "        r = self._target_img_size / min(h, w)\n",
    "        s = (round(r * h), round(r*w))\n",
    "        # print(f'New size: {s}')\n",
    "\n",
    "        img = cv2.resize(img, s, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "\n",
    "        # Center crop\n",
    "        x = w/2 - self._target_img_size/2\n",
    "        y = h/2 - self._target_img_size/2\n",
    "\n",
    "        crop_img = img[int(y):int(y+self._target_img_size),\n",
    "                       int(x):int(x+self._target_img_size)]\n",
    "        \n",
    "        # crop_img = crop_img.astype(np.float16)\n",
    "\n",
    "        # img = TF.center_crop(img, output_size=2 * [self._target_img_size])\n",
    "        # img = torch.unsqueeze(T.ToTensor()(crop_img), 0)\n",
    "\n",
    "        if self._transform:\n",
    "            img_tensor = self._transform(crop_img)\n",
    "        else:\n",
    "            img_tensor = crop_img\n",
    "\n",
    "        # img_tensor = self._to_tensor(img_tensor)\n",
    "\n",
    "        return img_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/home/jaswant/Documents/DiscreteVAE/data/Images\"\n",
    "\n",
    "mean_tensor = (0.4360, 0.4408, 0.4332)\n",
    "std_tensor = (0.2619, 0.2639, 0.2616)\n",
    "\n",
    "dog_data_normalized = DogDataset(data_folder=data_folder, transform=T.Compose([T.ToTensor(), T.Normalize(mean=mean_tensor, std=std_tensor)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_index = 178\n",
    "sample_dog = dog_data_normalized[sample_index]\n",
    "sample_dog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
    "        self._commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCHW -> BHWC\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "            \n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
